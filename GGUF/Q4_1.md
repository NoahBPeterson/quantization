# GGUF Q4_1 Quantization

## Before Quantization: Floating-Point Weights (BF16)

The layer consists of 2048 floating-point weights in BF16 (BFloat16) format:

Let’s say we have an array of floating-point weights (in BF16 format) of a layer before quantization:

    [1.25, -0.5, 2.3, -1.1, ..., 1.0, -0.9] <-- 2048 weights in total

 * Each BF16 weight is 16 bits (2 bytes).
 * Total size before quantization: 2048 × 2 = 4096 bytes.

## After Quantization: Q4_1 Format

In Q4_1, the weights are split into blocks, quantized to 4-bit values, and shared metadata (scale and offset) is stored per block.

Step 1: Divide into Blocks

 * The 2048 weights are divided into 64 blocks, each containing 32 weights.

This gives us:

    Block 1          Block 2          ...    Block 64
    [32 wts]         [32 wts]         ...    [32 wts] 

Step 2: Compute Scale and Offset for Each Block

For each block of 32 weights:

 * Min-Max Scaling:

    * Compute the minimum (min) and maximum (max) values of all weights in the block.

    * Compute the scaling factor (d) as:
      * `d = (max − min) / 15`

    * Store the offset (m) as the block-wide minimum value (min).

Step 3: Quantize Each Weight to 4 Bits

 * Normalize weights in the block using the scale (d) and offset (m): 
     * `x′ = (x − m) / d`

 * Round each normalized weight to fit in the 4-bit range [0,15]:

    * `quantized_weight = min(15, int(x′ + 0.5))`

Example:

    Original weights: [1.25, -0.5, 2.3, -1.1, ...]
    Quantized (4-bit) weights: [10, 3, 15, 0, ...]

Step 4: Pack Quantized Weights into Bytes

 * Each quantized weight is represented as a 4-bit value.
 * Two quantized weights are packed into one byte:
    * Lower 4 bits store one weight, and upper 4 bits store the next.

Example:

    4-bit values in Block 1: [10, 3, 15, 0, ...] Packed into bytes: [0xA3, 0xF0, ...]
Step 5: Store Block Metadata

 * For each block, the scaling factor (d) and offset (m) are stored as 16-bit FP16 values.

Example:

    d = 0.2 (scaled down to fit FP16)
    m = -1.0 (scaled down to fit FP16)

## Final Memory Layout

| Section |	Data |	Size per Block
|--------------|------------------------------------|------------|
| Quantized Weights |	Packed 4-bit values |	16 bytes |
| Block-Wide Scale (d) |	Block-wide scale in FP16 |	2 bytes |
| Block-Wide Offset (m) |	Block-wide minimum in FP16 |	2 bytes |
| Total per Block |		| 20 bytes |

## Summary for the Entire Layer

Since there are 64 blocks, the total quantized representation for the layer is:

| Section |	Data Size in Bytes |
|--------------|------------------------------------|
| Quantized Weights |	16x64=1024 bytes |
| Block Scales (d) |	2×64=128 bytes |
| Block Offsets (m) |	2×64=128 bytes |
| Total  |	1280 bytes |

## Comparison: Before and After Quantization
| Data |	Format |	Total Size in Bytes|
|--------------|------------------------------------|------------|
| Original Floating-Point Weights |	BF16 (16 bits) |	4096 bytes|
| Quantized Weights (Q4_1) |	Q4_1 (5 bits) |	1280 bytes

For Q4_1:

    Layer (2048 weights)
    ├── Block 1 (32 weights, block-wide scale and offset)
    │   ├── Weight 1
    │   ├── Weight 2
    │   ├── ...
    │   ├── Weight 32
    │   ├── Block-wide scale and offset
    ├── Block 2 (32 weights, block-wide scale and offset)
    ├── ...
    └── Block 64 (32 weights, block-wide scale and offset)

## Limitations

Q4_1 quantization requires the total number of weights in a layer to be divisible by 32 (the block size). Layers with non-divisible weight counts cannot be quantized using Q4_1.