# GGUF Q5_1 Quantization

## Before Quantization: Floating-Point Weights (BF16)

The layer consists of 2048 floating-point weights in BF16 (BFloat16) format.

For example, consider an array of floating-point weights for a layer before quantization:

    [1.25, -0.5, 2.3, -1.1, ..., 1.0, -0.9]  <-- 2048 weights in total

- Each BF16 weight occupies 16 bits (2 bytes).
- Total size before quantization: 2048 × 2 = 4096 bytes.

## After Quantization: Q5_1 Format

In Q5_1, the weights are split into blocks, quantized to 5-bit values, and two pieces of per-block metadata are stored:
- **Block-Wide Scale (d):** a 16-bit FP16 value.
- **Block-Wide Minimum (m):** a 16-bit FP16 value.

Additionally, each block contains packed quantized data:
- **Quantized Weights:** The 5-bit quantized values are stored in a packed format where the lower 4 bits are combined into a 16-byte array and the most significant (5th) bits are collected into a 32-bit integer.

For Q5_1, the dequantized weight is recovered using the relation:

    w = q * d + m

---

### Step 1: Divide into Blocks

- The 2048 weights are divided into 64 blocks; each block contains 32 weights.

This results in:

    Block 1          Block 2          ...    Block 64
    [32 wts]         [32 wts]         ...    [32 wts]

---

### Step 2: Compute Scale and Offset for Each Block

For each block of 32 weights:

1. **Determine the Minimum and Maximum:**
   - Compute the minimum value (min) in the block.
   - Compute the maximum value (max) in the block.

2. **Calculate the Scale Factor:**
   - The quantization range for 5 bits is 0 to 31.
   - Compute the block-wide scale:
     
         d = (max − min) / 31

3. **Store the Offset:**
   - The block-wide offset is the minimum value:
     
         m = min

4. **Precompute the Inverse Scale:**
   - Compute the inverse scale for use during quantization:
     
         id = 1/d  (if d is non-zero; otherwise id = 0)

5. **Store Metadata:**
   - Save **d** and **m** as FP16 values (2 bytes each).

---

### Step 3: Quantize Each Weight to 5 Bits

For each weight `x` in the block:

1. **Normalize the Weight:**
   - Compute the normalized value by subtracting the minimum and scaling:
     
         x' = (x − m) * id

2. **Round to the Nearest Integer:**
   - Apply rounding by adding 0.5:
     
         quantized_value = (uint8_t)(x' + 0.5)

3. **Clamp the Range:**
   - Ensure that the resulting quantized value is within the range [0, 31].

The quantized value represents the index in the quantization grid.

---

### Step 4: Pack Quantized Weights and Extract High Bits

The 32 quantized 5-bit values are processed in two halves (first 16 weights and the remaining 16 weights):

1. **Extract Lower 4 Bits:**
   - For each weight, extract its lower 4 bits by:
     
         low = quantized_value & 0x0F

2. **Extract the 5th (High) Bit:**
   - Extract the most significant (5th) bit using:
     
         high = (quantized_value & 0x10) >> 4

3. **Pack Lower 4 Bits into the `qs` Array:**
   - For indices `j` from 0 to 15:
     - From the first half (weights 0–15): obtain `lb0 = w[j] & 0x0F`
     - From the second half (weights 16–31): obtain `lb1 = w[j+16] & 0x0F`
     - Pack them into one byte with `lb0` in the lower nibble and `lb1` in the upper nibble:
       
           qs[j] = (lb0) | (lb1 << 4)

4. **Pack High Bits into a 32-Bit Integer (`qh`):**
   - Initialize a 32-bit integer `qh = 0`.
   - For each index `j` from 0 to 15:
     - Extract `hb0 = (w[j] & 0x10) >> 4` and place it at bit position `j`:
       
           qh |= hb0 << j
     - Extract `hb1 = (w[j+16] & 0x10) >> 4` and place it at bit position `j + 16`:
       
           qh |= hb1 << (j + 16)

After packing, the `qs` array occupies 16 bytes and `qh` occupies 4 bytes.

---

### Step 5: Store Block Metadata

Store the metadata and packed quantized weights for each block as follows:

- **Block-Wide Scale (`d`):** 2 bytes (FP16)
- **Block-Wide Minimum (`m`):** 2 bytes (FP16)
- **High Bits (`qh`):** 4 bytes
- **Packed Low Bits (`qs`):** 16 bytes

**Total per Block:** 24 bytes

---

## Summary for the Entire Layer

Since there are 64 blocks, the total quantized representation for the layer is:

| Section                    | Data Size in Bytes          |
|----------------------------|-----------------------------|
| Quantized Weights (qs)     | 16 bytes × 64 = 1024 bytes   |
| High Bits (qh)             | 4 bytes × 64 = 256 bytes     |
| Block Scales (d)           | 2 bytes × 64 = 128 bytes     |
| Block Offsets (m)          | 2 bytes × 64 = 128 bytes     |
| **Total**                  | **1024 + 256 + 128 + 128 = 1536 bytes** |

---

## Comparison: Before and After Quantization

| Data                                    | Format         | Total Size in Bytes |
|-----------------------------------------|----------------|---------------------|
| Original Floating-Point Weights         | BF16 (16 bits) | 4096 bytes          |
| Quantized Weights (Q5_1)                | Q5_1           | 1536 bytes          |

For Q5_1:

    Layer (2048 weights)
    ├── Block 1 (32 weights, block-wide scale and minimum)
    │   ├── Weight 1
    │   ├── Weight 2
    │   ├── ...
    │   ├── Weight 32
    │   ├── Block-wide scale (FP16)
    │   ├── Block-wide minimum (FP16)
    │   ├── High bits (packed 5th bits)
    ├── Block 2 (32 weights, block-wide scale and minimum)
    ├── ...
    └── Block 64 (32 weights, block-wide scale and minimum)

---

## Limitations

- Q5_1 quantization requires the total number of weights in a layer to be divisible by 32 (the block size).
- Layers with non-divisible weight counts cannot be quantized using Q5_1 without additional handling (e.g., padding or trimming).

This format uses round-to-nearest quantization.
