# GGUF Q5_0 Quantization

## Before Quantization: Floating-Point Weights (BF16)

The layer consists of 2048 floating-point weights in BF16 (BFloat16) format:

Let's say we have an array of floating-point weights of a layer before quantization:

    [1.25, -0.5, 2.3, -1.1, ..., 1.0, -0.9]  <-- 2048 weights in total

 * Each BF16 weight is 16 bits (2 bytes).
 * Total size before quantization: 2048 × 2 = 4096 bytes.

## After Quantization: Q5_0 Format

In Q5_0, the weights are split into blocks, quantized to 5-bit values, and the FP16 block-wide scale is stored for each block.

### Step 1: Divide into Blocks

* The 2048 weights are divided into 64 blocks, each containing 32 weights.

This gives us:

    Block 1          Block 2          ...    Block 64
    [32 wts]         [32 wts]         ...    [32 wts]

### Step 2: Compute Block-Wide Scale (d)

For each block of 32 weights:

* Find the weight with the maximum absolute value in the block. Let the corresponding weight (with its original sign) be denoted as `max`.
* Compute the scaling factor as:
    * `d = max / -16`
    * Compute the inverse scale: `id = 1/d` (if `d` is non-zero; otherwise, `id = 0`)
* Store the scale factor (`d`) as a 16-bit FP16 value.

### Step 3: Quantize Each Weight to 5 Bits

For each weight `x` in the block:

* Normalize the weight using the inverse scale:
    * `x_normalized = x * id`
* Compute the quantized value by adding an offset and rounding:
    * `quantized_value = round(x_normalized + 16.5)`
* Clamp the result to the range [0, 31].

Example:

    Original weights: [1.25, -0.5, 2.3, -1.1, ...]
    Quantized (5-bit) weights: [17, 6, 31, 2, ...]


### Step 4: Pack Quantized Weights and Extract High Bits

* Process the block in two halves (first 16 weights and the remaining 16 weights):
  * For each half, the lower 4 bits of the quantized value are packed together—two 4-bit values per byte.
  * Simultaneously, extract the 5th bit (the most significant bit) from each quantized value.
* The lower 4-bit values for all 32 weights are stored in an array (`qs`) of 16 bytes.
* The extracted 5th bits for all 32 weights are packed into a 32-bit integer (`qh`), occupying 4 bytes.

Example:
    Quantized weights: [17, 6, 31, 2, ...]
    Packed low bits: [0x11, 0x06, 0x1F, 0x02, ...]
    High bits: [0x01, 0x00, 0x1F, 0x00, ...]

### Step 5: Store Block Metadata

Each block stores the following metadata:
* **Block-Wide Scale (`d`):** 2 bytes (stored as FP16).
* **High Bits (`qh`):** 4 bytes.
* **Low bits (`qs`):** 16 bytes.
* **Total per Block:** 22 bytes.

## Summary for the Entire Layer

Since there are 64 blocks, the total quantized representation for the layer is:

| Section              | Data Size in Bytes         |
|----------------------|----------------------------|
| Low bits (qs)        | 16 bytes × 64 = 1024 bytes  |
| High bits (qh)       | 4 bytes × 64 = 256 bytes   |
| Block Scales (d)     | 2 bytes × 64 = 128 bytes   |
| **Total**            | **1024 + 256 + 128 = 1408 bytes** |

## Comparison: Before and After Quantization

| Data                                   | Format        | Total Size in Bytes |
|----------------------------------------|---------------|---------------------|
| Original Floating-Point Weights        | BF16 (16 bits)| 4096 bytes          |
| Quantized Weights (Q5_0)                | Q5_0          | 1408 bytes          |

For Q5_0:

    Layer (2048 weights)
    ├── Block 1 (32 weights, block-wide scale)
    │   ├── Weight 1
    │   ├── Weight 2
    │   ├── ...
    │   ├── Weight 32
    │   ├── Block-wide scale (FP16)
    │   ├── High bits (packed 5th bits)
    ├── Block 2 (32 weights, block-wide scale)
    ├── ...
    └── Block 64 (32 weights, block-wide scale)

## Limitations

* Q5_0 quantization requires the total number of weights in a layer to be divisible by 32 (the block size). Layers with non-divisible weight counts cannot be quantized using Q5_0.
* This format uses round-to-nearest quantization.
