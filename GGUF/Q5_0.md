# GGUF Q5_0 Quantization

## Before Quantization: Floating-Point Weights (BF16)

The layer consists of 2048 floating-point weights in BF16 (BFloat16) format:

Let's say we have an array of floating-point weights of a layer before quantization:

    [1.25, -0.5, 2.3, -1.1, ..., 1.0, -0.9]  <-- 2048 weights in total

 * Each BF16 weight is 16 bits (2 bytes).
 * Total size before quantization: 2048 × 2 = 4096 bytes.

## After Quantization: Q5_0 Format

In Q5_0, the weights are split into blocks, quantized to 5-bit values, and the FP16 block-wide scale is stored for each block.

### Step 1: Divide into Blocks

* The 2048 weights are divided into 64 blocks, each containing 32 weights.

This gives us:

    Block 1          Block 2          ...    Block 64
    [32 wts]         [32 wts]         ...    [32 wts]

### Step 2: Compute Block-Wide Scale (d)

For each block of 32 weights:

* Find the weight with the maximum absolute value in the block. Let the corresponding weight (with its original sign) be denoted as `max`.
* Compute the scaling factor as:
    * `d = max / -16`
    * Compute the inverse scale: `id = 1/d` (if `d` is non-zero; otherwise, `id = 0`)
* Store the scale factor (`d`) as a 16-bit FP16 value.

### Step 3: Quantize Each Weight to 5 Bits

For each weight `x` in the block:

* Normalize the weight using the inverse scale:
    * `x_normalized = x * id`
* Compute the quantized value by adding an offset and rounding:
    * `quantized_value = round(x_normalized + 16.5)`
* Clamp the result to the range [0, 31].

These quantized values seed the following packed bits (see Step 4):

- **Packed low bits (qs):**  
  Computed by packing the lower 4 bits of weights:
  
  ```
  qs[0]  = ( (weights[16] & 0x0F) << 4 ) | (weights[0]  & 0x0F)  = (7  << 4) | 1  = 0x71
  qs[1]  = ( (weights[17] & 0x0F) << 4 ) | (weights[1]  & 0x0F)  = (6  << 4) | 6  = 0x66
  qs[2]  = ( (weights[18] & 0x0F) << 4 ) | (weights[2]  & 0x0F)  = (2  << 4) | 15 = 0x2F
  qs[3]  = ( (weights[19] & 0x0F) << 4 ) | (weights[3]  & 0x0F)  = (1  << 4) | 2  = 0x12
  ...
  ```
  Final packed low bits array:
  
  ```
  qs = [0x71, 0x66, 0x2F, 0x12, 0x05, 0xF3, 0xE0, 0xDE,
        0xCF, 0xEE, 0xDD, 0xCC, 0xBB, 0xAA, 0x99, 0x88]
  ```

- **Packed high bits (qh):**  
  Computed by extracting and packing the 5th bit of each weight:
  
  The first half (weights[0] to weights[15]) yields:
  
  ```
  [1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]  → Lower 16 bits: 0x0085
  ```
  
  The second half (weights[16] to weights[31]) yields:
  
  ```
  [0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]  → Upper 16 bits: 0xFE1C
  ```
  
  Combining both gives:
  
  ```
  qh = 0xFE1C0085
  ```

### Step 4: Pack Quantized Weights and Extract High Bits

Each 5-bit quantized weight is split into:
- **Lower 4 Bits:** `low = quantized_value & 0x0F`
- **5th (High) Bit:** `high = (quantized_value & 0x10) >> 4`

The packing is performed in two halves (indices 0–15 for the first half; 16–31 for the second half):

1. **Pack Lower 4 Bits into `qs`:**
   For each index `j` from 0 to 15:
   - `lb0 = weights[j] & 0x0F`
   - `lb1 = weights[j+16] & 0x0F`
   - Then pack into one byte:
     
         qs[j] = (lb1 << 4) | lb0

2. **Pack High Bits into a 32-Bit Integer (`qh`):**
   Initialize `qh = 0`. Then for each index `j` from 0 to 15:
   - Extract high bit from first half:
     
         hb0 = (weights[j] & 0x10) >> 4
     
     and set:
     
         qh |= hb0 << j
     
   - Extract high bit from second half:
     
         hb1 = (weights[j+16] & 0x10) >> 4
     
     and set:
     
         qh |= hb1 << (j + 16)

### Step 5: Store Block Metadata

Each block stores the following metadata:
* **Block-Wide Scale (`d`):** 2 bytes (stored as FP16).
* **High Bits (`qh`):** 4 bytes.
* **Low bits (`qs`):** 16 bytes.
* **Total per Block:** 22 bytes.

## Summary for the Entire Layer

Since there are 64 blocks, the total quantized representation for the layer is:

| Section              | Data Size in Bytes         |
|----------------------|----------------------------|
| Low bits (qs)        | 16 bytes × 64 = 1024 bytes  |
| High bits (qh)       | 4 bytes × 64 = 256 bytes   |
| Block Scales (d)     | 2 bytes × 64 = 128 bytes   |
| **Total**            | **1024 + 256 + 128 = 1408 bytes** |

## Comparison: Before and After Quantization

| Data                                   | Format        | Total Size in Bytes |
|----------------------------------------|---------------|---------------------|
| Original Floating-Point Weights        | BF16 (16 bits)| 4096 bytes          |
| Quantized Weights (Q5_0)                | Q5_0 (5.5 bits)         | 1408 bytes          |

For Q5_0:

    Layer (2048 weights)
    ├── Block 1 (32 weights, block-wide scale)
    │   ├── Weight 1
    │   ├── Weight 2
    │   ├── ...
    │   ├── Weight 32
    │   ├── Block-wide scale (FP16)
    ├── Block 2 (32 weights, block-wide scale)
    ├── ...
    └── Block 64 (32 weights, block-wide scale)

## Limitations

* Q5_0 quantization requires the total number of weights in a layer to be divisible by 32 (the block size). Layers with non-divisible weight counts cannot be quantized using Q5_0.
* This format uses round-to-nearest quantization.
