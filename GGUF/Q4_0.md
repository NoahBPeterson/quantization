# GGUF Q4_0 Quantization

## Before Quantization: Floating-Point Weights (BF16)

The layer consists of 2048 floating-point weights in BF16 (BFloat16) format:

Let’s say we have an array of floating-point weights (in BF16 format) of a layer before quantization:

    [1.25, -0.5, 2.3, -1.1, ..., 1.0, -0.9] <-- 2048 weights in total

 * Each BF16 weight is 16 bits (2 bytes).
 * Total size before quantization: 2048 × 2 = 4096 bytes.

## After Quantization: Q4_0 Format

In Q4_0, the weights are split into blocks, quantized to 4-bit values, and the shared metadata is stored per block.

Step 1: Divide into Blocks

 * The 2048 weights are divided into 64 blocks, each containing 32 weights.

This gives us:

    Block 1          Block 2          ...    Block 64
    [32 wts]         [32 wts]         ...    [32 wts]

Step 2: Compute Scale for Each Block

For each block of 32 weights:

 * Absolute Maximum Scaling:
    * Compute the absolute maximum value (absmax) of all weights in the block.
    * Compute the scaling factor (d) as: `d = absmax / -8`

    * The inverse scale `id` is precomputed for quantization: `id = 1/d`

Step 3: Quantize Each Weight to 4 Bits

 * Normalize weights in the block using the inverse scale:
    * `x′ = x * id`

 * Shift and round each normalized weight to fit in the 4-bit range [0,15]:

    * `quantized_weight = min(15, int(x′ + 8.5))`

Example:

    Original weights: [1.25, -0.5, 2.3, -1.1, ...]
    Quantized (4-bit) weights: [10, 3, 15, 0, ...]

Step 4: Pack Quantized Weights into Bytes

 * Each quantized weight is represented as a 4-bit value.
 * Two quantized weights are packed into one byte:
   * Lower 4 bits store one weight, and upper 4 bits store the next.

Example:

    4-bit values in Block 1: [10, 3, 15, 0, ...]
    Packed into bytes: [0xA3, 0xF0, ...]

Step 5: Store Block Metadata

 * For each block:
    * The scale factor (d) is stored as 16-bit FP16.

Example:

    d = 0.2 (scaled down to fit FP16)

## Final Memory Layout

| Section |	Data |	Size per Block
|--------------|------------------------------------|------------|
| Quantized Weights |	Packed 4-bit values |	16 bytes |
| Block-Wide Scale (d) |	Block-wide scale in FP16 |	2 bytes |
| Total per Block |		| 18 bytes |

## Summary for the Entire Layer

Since we have 64 blocks, the total quantized representation for the layer is:

| Section |	Data Size in Bytes |
|--------------|------------------------------------|
| Quantized Weights |	16x64=1024 bytes |
| Block Scales |	2×64=128 bytes |
| Total  |	1152 bytes |


## Comparison: Before and After Quantization
| Data |	Format |	Total Size in Bytes|
|--------------|------------------------------------|------------|
| Original Floating-Point Weights |	BF16 (16 bits) |	4096 bytes|
| Quantized Weights (Q4_0) |	Q4_0 |	1152 bytes

For Q4_0:

    Layer (2048 weights)
    ├── Block 1 (32 weights, block-wide scale)
    │   ├── Weight 1
    │   ├── Weight 2
    │   ├── ...
    │   ├── Weight 32
    │   ├── Block-wide scale
    ├── Block 2 (32 weights, block-wide scale)
    ├── ...
    └── Block 64 (32 weights, block-wide scale)

## Limitations

Q4_0 quantization requires the total number of weights in a layer to be divisible by 32 (the block size). Layers with non-divisible weight counts cannot be quantized using Q4_0.